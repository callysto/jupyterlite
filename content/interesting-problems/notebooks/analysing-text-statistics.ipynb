{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Finteresting-problems&branch=main&subPath=notebooks/analysing-text-statistics.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Text Statistics\n",
    "\n",
    "Let's try out some statistical analysis of text, including [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), using a [public domain](https://en.wikipedia.org/wiki/Public_domain) book from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    "The example we'll use is the [most downloaded](http://www.gutenberg.org/browse/scores/top) book, [Pride and Prejudice by Jane Austen](http://www.gutenberg.org/ebooks/1342). Running this first code cell will import and display the contents of the book.\n",
    "\n",
    "Feel free to change the link in the following code cell if you'd like to explore another book, but make sure you are using the `Plain Text UTF-8` link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_text_link = 'https://www.gutenberg.org/cache/epub/42671/pg42671.txt'\n",
    "\n",
    "import requests\n",
    "r = requests.get(gutenberg_text_link) # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "book_title = r.text[r.text.index('Title:')+7:r.text.index('Author:')-4] # find the book title\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a DataFrame of Chapters\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text and length of each chapter (number of characters, including spaces and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ch = [] # create an empty list that we will append to\n",
    "for chapter in text.split('CHAPTER'):\n",
    "    if len(chapter)>500: # so that we are getting actual book chapters\n",
    "        chapter = chapter.replace('\\r','').replace('\\n',' ') # replace the 'new line' characters with spaces\n",
    "        ch.append(chapter) # append the chapter text to the list\n",
    "book = pd.DataFrame(ch, columns=['Chapter Text']) # create a dataframe from the list\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 0 is the introduction and transcriber's note, so we'll eliminate that, and create a column called `Chapter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = book.drop(0).reset_index().rename(columns={'index':'Chapter'})\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the `Chapter Text` begins with a roman numeral of the chapter number, let's remove that by splitting the text from the first `.` and stripping off extra spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Chapter Text'] = book['Chapter Text'].apply(lambda x: x.split('.', 1)[1].strip())\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "From that dataframe we can calculate chapter length (number of characters) and create a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "book['Length'] = book['Chapter Text'].apply(len) # add a column with the length of each chapter\n",
    "px.bar(book, x='Chapter', y='Length', title='Characters per Chapter in '+book_title).update_xaxes(title='Chapter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text. We'll create a new column in our dataframe containing the processed text.\n",
    "\n",
    "This will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following two installation lines if you get errors\n",
    "#!pip install spacy --user\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "\n",
    "def processLanguage(chapter):\n",
    "    parts_of_speech_list = [] # create an empty list\n",
    "    for token in nlp(chapter):\n",
    "        if token.is_alpha: # if the token is a word\n",
    "            parts_of_speech_list.append(token)\n",
    "    return parts_of_speech_list\n",
    "\n",
    "book['NLP'] = book['Chapter Text'].apply(processLanguage)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've applied some natural language processing to identify the words in each chapter, let's see how many verbs, adjectives, nouns, and proper nouns there are. We'll also calculate word counts and proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Word Count'] = book['NLP'].apply(len)\n",
    "book['Verbs'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'VERB'])\n",
    "book['Verb Count'] = book['Verbs'].apply(len)\n",
    "book['Verb %'] = book['Verb Count']/book['Word Count']*100\n",
    "book['Adjectives'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'ADJ'])\n",
    "book['Adjective Count'] = book['Adjectives'].apply(len)\n",
    "book['Adjective %'] = book['Adjective Count']/book['Word Count']*100\n",
    "book['Nouns'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'NOUN'])\n",
    "book['Noun Count'] = book['Nouns'].apply(len)\n",
    "book['Noun %'] = book['Noun Count']/book['Word Count']*100\n",
    "book['Proper Nouns'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'PROPN'])\n",
    "book['Proper Noun Count'] = book['Proper Nouns'].apply(len)\n",
    "book['Proper Noun %'] = book['Proper Noun Count']/book['Word Count']*100\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the proportional occurrences of those types of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(book, x='Chapter', y=['Verb %','Adjective %','Noun %','Proper Noun %'], title='Proportion of Word Types in '+book_title).update_xaxes(title='Chapter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Verbs\n",
    "\n",
    "To get an idea of the most common words in the text we can look at a part of speech, verbs for example, and count which are the most frequent. We'll display the `10` most common verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def countLemma(row):\n",
    "    return Counter([token.lemma_.strip().lower() for token in row])\n",
    "book['Verb Lemmas'] = book['Verbs'].apply(countLemma)\n",
    "verb_counter = Counter()\n",
    "for row in book['Verb Lemmas']:\n",
    "    verb_counter.update(row)\n",
    "verb_counter.most_common(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Names\n",
    "\n",
    "The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names). We can look at the character names and how often they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(row):\n",
    "    return Counter([token.lemma_.strip() for token in row])\n",
    "book['Names'] = book['Proper Nouns'].apply(nameCounter)\n",
    "name_counter = Counter()\n",
    "for row in book['Names']:\n",
    "    name_counter.update(row)\n",
    "names_df = pd.DataFrame.from_dict(name_counter, orient='index', columns=['Count'])\n",
    "names_df.sort_values('Count', ascending=False, inplace=True)\n",
    "names_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "If you'd like to remove nouns that may categorized incorrectly or are uncommon, we can drop rows with fewer than `15` occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = names_df[names_df['Count'] > 14]\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Name Frequencies\n",
    "\n",
    "Let's make a bar graph of the top `20` most frequently mentioned characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(names_df.head(20), title='Name Frequencies in '+book_title).update_yaxes(title='Frequency').update_xaxes(title='Name').update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize the mentions of the top `5` character names throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(name):\n",
    "    name_counts = []\n",
    "    for row in book['Proper Nouns']:\n",
    "        n = 0\n",
    "        for token in row:\n",
    "            if token.text == name:\n",
    "                n += 1\n",
    "        name_counts.append(n)\n",
    "    return(name_counts)\n",
    "\n",
    "main_characters_list = names_df.head(5).index.to_list()\n",
    "main_characters = pd.DataFrame()\n",
    "for character in main_characters_list:\n",
    "    main_characters[character] = nameCounter(character)\n",
    "    main_characters[character+' Total'] = main_characters[character].cumsum()\n",
    "main_characters['Chapter'] = main_characters.index+1\n",
    "px.line(main_characters, x='Chapter', y=main_characters_list, title='Main Character Frequencies in '+book_title).update_yaxes(title='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [character+' Total' for character in main_characters_list]\n",
    "px.line(main_characters, x='Chapter', y=y, title='Cumulative Main Character Frequencies in '+book_title).update_yaxes(title='Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive).\n",
    "\n",
    "For this we will use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, then visualize the positive and negative sentiment by chapter.\n",
    "\n",
    "This will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment --user\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "book['Sentiment'] = book['Chapter Text'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "book['Positive Sentiment'] = book['Sentiment'].str['pos']\n",
    "book['Negative Sentiment'] = -book['Sentiment'].str['neg']\n",
    "px.bar(book, x='Chapter',  y=['Positive Sentiment', 'Negative Sentiment'], title=book_title+' Sentiment Analysis by Chapter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "One last library to introduce, [textstat](https://github.com/shivam5992/textstat) for checking the readability, complexity, and grade level of text. It includes a [number of functions](https://github.com/shivam5992/textstat#list-of-functions), but we'll only use a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user textstat\n",
    "import textstat\n",
    "book['Flesch-Kincaid Grade'] = book['Chapter Text'].apply(lambda x:textstat.flesch_kincaid_grade(x))\n",
    "book['Gunning Fog Index'] = book['Chapter Text'].apply(lambda x:textstat.gunning_fog(x))\n",
    "book['Linsear Write Formula'] = book['Chapter Text'].apply(lambda x:textstat.linsear_write_formula(x))\n",
    "book['Readability'] = book['Chapter Text'].apply(lambda x:textstat.text_standard(x, float_output=True))\n",
    "px.line(book, x='Chapter', y=['Flesch-Kincaid Grade','Gunning Fog Index','Linsear Write Formula','Readability'], title='Readability Scores for '+book_title).update_xaxes(title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully this was an interesting introduction to text statistics such as chapter length, word frequency and word type frequency, sentiment analysis, and reading levels. You can, of course, use this analyse the text from any other online document in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
