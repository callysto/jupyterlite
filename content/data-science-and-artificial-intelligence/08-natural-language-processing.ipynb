{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "[![what is NLP](https://img.youtube.com/vi/oi0JXuL19TA/0.jpg)](https://www.youtube.com/watch?v=oi0JXuL19TA)\n",
    "\n",
    "\n",
    "One type of [discriminative](https://en.wikipedia.org/wiki/Discriminative_model) [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) is [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing).\n",
    "\n",
    "Natural language processing (NLP) helps computers understand human languages. It allows computers to read and analyze text, understand speech, and even generate human-like responses. With NLP, computers can translate languages, summarize articles, and even recognize emotions in text or speech. NLP has become an important technology in many areas, including social media, healthcare, and education. It helps people communicate more effectively with computers, making it easier for them to get information and complete tasks.\n",
    "\n",
    "In this notebook we will use NLP to analyze the text of a book. We'll choose a book that is in the [public domain](https://en.wikipedia.org/wiki/Public_domain), meaning there are no restrictions on how it can be used.\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/) is a great source for public domain books. We will want to use the `Plain Text UTF-8` version of a book. We'll start with [The Call of the Wild by Jack London](https://www.gutenberg.org/ebooks/215).\n",
    "\n",
    "<img src=\"https://www.gutenberg.org/files/215/215-h/images/cover.jpg\" align=\"center\" width=\"200\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url = 'https://www.gutenberg.org/cache/epub/215/pg215.txt'\n",
    "\n",
    "#%pip install -q pyodide_http spacey\n",
    "import piplite, pyodide, pyodide_http\n",
    "await piplite.install(['nbformat','plotly','requests','statsmodels', 'spacey'])\n",
    "pyodide_http.patch_all()\n",
    "import requests\n",
    "r = requests.get(book_url)\n",
    "r.encoding = 'utf-8'\n",
    "data = r.text\n",
    "data = data.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace smart quotes with regular quotation marks\n",
    "print(data[:700]) # print the first 700 characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the full text of the book stored in the variable `book`, but we notice that there is some extra content at the start and end that we want to get rid of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split('Contents')[1] # remove the first part of the book\n",
    "data = data.split('End of the Project Gutenberg EBook')[0] # remove the last part of the book\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book uses the word `Chapter` to show the start of each chapter, so we can split it into a list of chapters.\n",
    "\n",
    "There are also line breaks (`\\r\\n`) included, we can replace them with spaces. We can also strip any leading or trailing spaces in each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = data.split('Chapter ')\n",
    "chapters = [chapter.replace('\\r\\n',' ') for chapter in chapters]\n",
    "chapters = [chapter.strip() for chapter in chapters]\n",
    "chapters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing Chapters ##\n",
    "Now that we have cleaned up the text, we can split it into a list of chapters to make it easier to work with. We will create a dataframe of the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(chapters, columns=['Text'])\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the first few rows aren't chapters but are just the table of contents. We we want just the rows from `8` to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[8:].copy()\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also reindex the dataframe so that it starts from `0` instead of `8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `text` column also includes the chapter titles, which we want to remove. We can use items `1` to `7` of our `chapters` list to get the chapter titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Title'] = chapters[1:8]\n",
    "for i in range(len(data)):\n",
    "    data['Text'][i] = data['Text'][i].replace(data['Title'][i], '')\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Now we can finally do some NLP. We'll use the [spaCy](https://spacy.io) natural language processing library to process the text and create new columns in our dataframe containing the identified sentences and words. This will allow us to summarize the text and perform some other analysis.\n",
    "\n",
    "The first few lines of the following code cell set up spaCy and define a `processLanguage` function. It may take a little while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    %python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def processLanguage(chapter):\n",
    "    processed = nlp(chapter)\n",
    "    sentences = list(processed.sents)\n",
    "    words = [] # create an empty list\n",
    "    for token in processed:\n",
    "        if token.is_alpha: # if the token is a word\n",
    "            words.append(token)\n",
    "    return sentences, words\n",
    "\n",
    "data['Sentences'], data['NLP'] = zip(*data['Text'].apply(processLanguage))\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts\n",
    "\n",
    "Since the new `NLP` column is a list of the words in a chapter, we can visualize the number of words per chapter.\n",
    "\n",
    "Remember that dataframes use zero-based indexing, so chapters 1 to 7 are labelled as 0 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['WordCount'] = data['NLP'].apply(lambda x: len(x))\n",
    "\n",
    "import plotly.express as px\n",
    "px.bar(data, y='WordCount', title='Word Count by Chapter').update_xaxes(title='Chapter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Text\n",
    "\n",
    "#### Stop Words\n",
    "\n",
    "To summarize each chapter, we'll start by removing any [stop words](https://en.wikipedia.org/wiki/Stop_word). Stop words are words such as `a`, `it`, `the`, or `and` that don't really add any meaning.\n",
    "\n",
    "Then if we find the frequencies of the signficant words in each each chapter, we will be able to weight each sentence based on how many of those it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(chapter):\n",
    "    chapter = [word for word in chapter if word.is_stop==False]\n",
    "    return chapter\n",
    "data['SignificantWords'] = data['NLP'].apply(removeStopWords)\n",
    "\n",
    "from collections import Counter\n",
    "def wordFrequencyCounter(chapter):\n",
    "    words = [word.text for word in chapter]\n",
    "    word_frequencies = Counter(words).most_common()\n",
    "    max_frequency = word_frequencies[0][1]\n",
    "    for w in range(len(word_frequencies)):\n",
    "        word_frequencies[w] = (word_frequencies[w][0], word_frequencies[w][1]/max_frequency) # normalize the word counts to values between 0 and 1\n",
    "    return word_frequencies\n",
    "\n",
    "data['SignificantWordFrequencies'] = data['SignificantWords'].apply(wordFrequencyCounter)\n",
    "print('Our data columns are now:', data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Chapter Summaries\n",
    "\n",
    "First we will weight the sentences by the frequencies of the significant words, then we'll record the 5% of the sentences that have the highest scores. Again, this might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "summaries = []\n",
    "for i in range(len(data)):\n",
    "    sentences = data['Sentences'][i]\n",
    "    number_to_output = int(len(sentences)*0.05) # 5% of the sentences\n",
    "    word_frequencies = data['SignificantWordFrequencies'][i]\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_score = 0\n",
    "        for word in sentence:\n",
    "            for word_frequency in word_frequencies:\n",
    "                if word.pos_ != 'PROPN': # if it is not a proper noun\n",
    "                    if word.text == word_frequency[0]:\n",
    "                        sentence_score += word_frequency[1]\n",
    "        sentence_scores[sentence] = sentence_score\n",
    "    summary = nlargest(number_to_output, sentence_scores, key=sentence_scores.get)\n",
    "    summaries.append(summary)\n",
    "data['Summary'] = summaries\n",
    "print('Summarization complete')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a basic chapter summary using the syntax `book['Summary'][x]` where `x` is the chapter index number (from 0 to 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Summary'][2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive). For this we will use [Vader Sentiment from the Natural Language Toolkit](https://www.nltk.org/_modules/nltk/sentiment/vader.html).\n",
    "\n",
    "Running the cell below will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "data['Sentiment'] = data['Text'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "data['PositiveSentiment'] = data['Sentiment'].str['pos']\n",
    "data['NegativeSentiment'] = -data['Sentiment'].str['neg']\n",
    "\n",
    "print('Our data columns are now:')\n",
    "for column in data.columns:\n",
    "    print(column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a visualization of the sentiment by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(data, y=['PositiveSentiment', 'NegativeSentiment'], title='Sentiment by Chapter').update_xaxes(title_text='Chapter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "Next we'll identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text to see how many verbs, adjectives, nouns, and proper nouns there are. We'll also calculate word count percents so we can make some graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech = {'Verbs':'VERB', 'Adjectives':'ADJ', 'Nouns':'NOUN', 'ProperNouns':'PROPN'}\n",
    "for part in parts_of_speech:\n",
    "    data[part] = data['NLP'].apply(lambda x: [token for token in x if token.pos_ == parts_of_speech[part]])\n",
    "    data[part+'Count'] = data[part].apply(len)\n",
    "    data[part+'%'] = data[part+'Count']/data['WordCount']*100\n",
    "\n",
    "px.line(data, x=data.index, y=['Verbs%', 'Adjectives%', 'Nouns%', 'ProperNouns%'], title='Parts of Speech by Chapter').update_xaxes(title='Chapter').update_yaxes(title='Percentage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Proper Nouns\n",
    "\n",
    "The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names). We can look at the character names and how often they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(row):\n",
    "    return Counter([token.lemma_.strip() for token in row])\n",
    "data['Names'] = data['ProperNouns'].apply(nameCounter)\n",
    "\n",
    "name_counter = Counter()\n",
    "for row in data['Names']:\n",
    "    name_counter.update(row)\n",
    "names_df = pd.DataFrame.from_dict(name_counter, orient='index', columns=['Count'])\n",
    "names_df = names_df.sort_values('Count')\n",
    "names_df = names_df[names_df['Count'] > 3] # only keep names that appear less than 3 times\n",
    "\n",
    "px.bar(names_df, x='Count', y=names_df.index, orientation='h', title='Proper Noun Frequencies', height=1000).update_xaxes(title='Count')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize the mentions of the top `5` character names throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(name):\n",
    "    name_counts = []\n",
    "    for row in data['ProperNouns']:\n",
    "        n = 0\n",
    "        for token in row:\n",
    "            if token.text == name:\n",
    "                n += 1\n",
    "        name_counts.append(n)\n",
    "    return(name_counts)\n",
    "\n",
    "main_characters_list = names_df.tail(5).index.to_list()\n",
    "main_characters = pd.DataFrame()\n",
    "for character in main_characters_list:\n",
    "    main_characters[character] = nameCounter(character)\n",
    "    main_characters[character+'Total'] = main_characters[character].cumsum()\n",
    "main_characters['Chapter'] = main_characters.index+1\n",
    "px.line(main_characters, y=main_characters_list, title='Main Character Frequencies').update_yaxes(title='Frequency').update_xaxes(title='Chapter').show()\n",
    "px.line(main_characters, y=[character+'Total' for character in main_characters_list], title='Cumulative Main Character Frequencies').update_yaxes(title='Cumulative Frequency').update_xaxes(title='Chapter').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:#663399\">Your **assignment** is to add at least two of these visualizations to a document and write about what you notice in them.</span>\n",
    "\n",
    "Some suggested visualizations are:\n",
    "* top 10 most common proper nouns\n",
    "* different parts of speech\n",
    "* cumulative mentions of main characters\n",
    "\n",
    "---\n",
    "\n",
    "That was a basic introduction to discriminative artificial intelligence and natural language processing. The [next notebook](09-generative-ai.ipynb) will introduce generative artificial intelligence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
